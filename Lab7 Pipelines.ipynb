{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Azure ML Pipelines\n",
    "<img src='https://github.com/retkowsky/images/blob/master/AzureMLservicebanniere.png?raw=true'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Azure Machine Learning Pipelines for Batch Inference\n",
    "\n",
    "In this notebook, we will demonstrate how to make predictions on large quantities of data asynchronously using the ML pipelines with Azure Machine Learning. Batch inference (or batch scoring) provides cost-effective inference, with unparalleled throughput for asynchronous applications. Batch prediction pipelines can scale to perform inference on terabytes of production data. Batch prediction is optimized for high throughput, fire-and-forget predictions for a large collection of data.\n",
    "\n",
    "> **Tip**\n",
    "If your system requires low-latency processing (to process a single document or small set of documents quickly), use [real-time scoring](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service) instead of batch prediction.\n",
    "\n",
    "In this example will be take a digit identification model already-trained on MNIST dataset using the [AzureML training with deep learning example notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb), and run that trained model on some of the MNIST test images in batch.  \n",
    "\n",
    "The input dataset used for this notebook differs from a standard MNIST dataset in that it has been converted to PNG images to demonstrate use of files as inputs to Batch Inference. A sample of PNG-converted images of the MNIST dataset were take from [this repository](https://github.com/myleott/mnist_png). \n",
    "\n",
    "The outline of this notebook is as follows:\n",
    "\n",
    "- Create a DataStore referencing MNIST images stored in a blob container.\n",
    "- Register the pretrained MNIST model into the model registry. \n",
    "- Use the registered model to do batch inference on the images in the data blob container.\n",
    "\n",
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first. This sets you up with a working config file that has information on your workspace, subscription id, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "Create a workspace object from the existing workspace. Workspace.from_config() reads the file config.json and loads the details into an object named ws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2021-03-22 12:52:32.244246\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print('Date:', now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using Azure ML 1.24.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(\"You are using Azure ML\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: AMLworkshop\n",
      "Azure region: westeurope\n",
      "Resource group: AMLworkshop-rg\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes. If the AmlCompute with that name is already in your workspace the code will skip the creation process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a new compute target...\n",
      "Creating....\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-03-22T12:52:38.313000+00:00', 'errors': None, 'creationTime': '2021-03-22T12:52:35.155686+00:00', 'modifiedTime': '2021-03-22T12:52:50.878493+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT18000S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "import random\n",
    "randomval=round(random.random()*1000000)\n",
    "compute_name=\"pipelines\"+str(randomval)\n",
    "compute_name\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = 0, \n",
    "                                                                max_nodes = 4,\n",
    "                                                                idle_seconds_before_scaledown=18000)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a datastore containing sample images\n",
    "The input dataset used for this notebook differs from a standard MNIST dataset in that it has been converted to PNG images to demonstrate use of files as inputs to Batch Inference. A sample of PNG-converted images of the MNIST dataset were take from [this repository](https://github.com/myleott/mnist_png).\n",
    "\n",
    "We have created a public blob container `sampledata` on an account named `pipelinedata`, containing these images from the MNIST dataset. In the next step, we create a datastore with the name `images_datastore`, which points to this blob container. In the call to `register_azure_blob_container` below, setting the `overwrite` flag to `True` overwrites any datastore that was created previously with that name. \n",
    "\n",
    "This step can be changed to point to your blob container by providing your own `datastore_name`, `container_name`, and `account_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "account_name = \"pipelinedata\"\n",
    "datastore_name = \"mnist_datastore\"\n",
    "container_name = \"sampledata\"\n",
    "\n",
    "mnist_data = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=datastore_name, \n",
    "                      container_name=container_name, \n",
    "                      account_name=account_name,\n",
    "                      overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's specify the default datastore for the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_data_store = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a FileDataset\n",
    "A [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) references single or multiple files in your datastores or public urls. The files can be of any format. FileDataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred.\n",
    "You can use dataset objects as inputs. Register the datasets to the workspace if you want to reuse them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "mnist_ds_name = 'mnist_sample_data'\n",
    "\n",
    "path_on_datastore = mnist_data.path('mnist')\n",
    "input_mnist_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dataset can be specified as a pipeline parameter, so that you can pass in new data when rerun the PRS pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "pipeline_param = PipelineParameter(name=\"mnist_param\", default_value=input_mnist_ds)\n",
    "input_mnist_ds_consumption = DatasetConsumptionConfig(\"minist_param_config\", pipeline_param).as_mount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data\n",
    "Intermediate data (or output of a Step) is represented by [PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py) object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "output_dir = PipelineData(name=\"inferences\", datastore=def_data_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Model\n",
    "\n",
    "Download and extract the model from https://pipelinedata.blob.core.windows.net/mnist-model/mnist-tf.tar.gz to \"models\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grid_29.pkl',\n",
       " 'grid_30.pkl',\n",
       " 'grid_31.pkl',\n",
       " 'grid_32.pkl',\n",
       " 'grid_33.pkl',\n",
       " 'grid_34.pkl',\n",
       " 'grid_35.pkl',\n",
       " 'mnist-tf.model.data-00000-of-00001',\n",
       " 'mnist-tf.model.index',\n",
       " 'mnist-tf.model.meta',\n",
       " 'saved_model.pb',\n",
       " 'unmitigated.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "# create directory for model\n",
    "model_dir = 'models'\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "url=\"https://pipelinedata.blob.core.windows.net/mnist-model/mnist-tf.tar.gz\"\n",
    "response = urllib.request.urlretrieve(url, \"model.tar.gz\")\n",
    "tar = tarfile.open(\"model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(model_dir)\n",
    "\n",
    "os.listdir(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model with Workspace\n",
    "A registered model is a logical container for one or more files that make up your model. For example, if you have a model that's stored in multiple files, you can register them as a single model in the workspace. After you register the files, you can then download or deploy the registered model and receive all the files that you registered.\n",
    "\n",
    "Using tags, you can track useful information such as the name and version of the machine learning library used to train the model. Note that tags must be alphanumeric. Learn more about registering models [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#registermodel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model mnist-prs\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "# register downloaded model \n",
    "model = Model.register(model_path=\"models/\",\n",
    "                       model_name=\"mnist-prs\", # this is the name the model is registered as\n",
    "                       tags={'pretrained': \"mnist\"},\n",
    "                       description=\"Mnist trained tensorflow model\",\n",
    "                       workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mnist-prs:23'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your model to make batch predictions\n",
    "To use the model to make batch predictions, you need an **entry script** and a list of **dependencies**:\n",
    "\n",
    "#### An entry script\n",
    "This script accepts requests, scores the requests by using the model, and returns the results.\n",
    "- __init()__ - Typically this function loads the model into a global object. This function is run only once at the start of batch processing per worker node/process. Init method can make use of following environment variables (ParallelRunStep input):\n",
    "    1.\tAZUREML_BI_OUTPUT_PATH â€“ output folder path\n",
    "- __run(mini_batch)__ - The method to be parallelized. Each invocation will have one minibatch.<BR>\n",
    "__mini_batch__: Batch inference will invoke run method and pass either a list or Pandas DataFrame as an argument to the method. Each entry in min_batch will be - a filepath if input is a FileDataset, a Pandas DataFrame if input is a TabularDataset.<BR>\n",
    "__run__ method response: run() method should return a Pandas DataFrame or an array. For append_row output_action, these returned elements are appended into the common output file. For summary_only, the contents of the elements are ignored. For all output actions, each returned output element indicates one successful inference of input element in the input mini-batch.\n",
    "    User should make sure that enough data is included in inference result to map input to inference. Inference output will be written in output file and not guaranteed to be in order, user should use some key in the output to map it to input.\n",
    "    \n",
    "\n",
    "#### Dependencies\n",
    "Helper scripts or Python/Conda packages required to run the entry script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Copyright (c) Microsoft. All rights reserved.\n",
      "# Licensed under the MIT license.\n",
      "\n",
      "import os\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from PIL import Image\n",
      "from azureml.core import Model\n",
      "\n",
      "\n",
      "def init():\n",
      "    global g_tf_sess\n",
      "\n",
      "    # pull down model from workspace\n",
      "    model_path = Model.get_model_path(\"mnist-prs\")\n",
      "\n",
      "    # contruct graph to execute\n",
      "    tf.reset_default_graph()\n",
      "    saver = tf.train.import_meta_graph(os.path.join(model_path, 'mnist-tf.model.meta'))\n",
      "    g_tf_sess = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\n",
      "    saver.restore(g_tf_sess, os.path.join(model_path, 'mnist-tf.model'))\n",
      "\n",
      "\n",
      "def run(mini_batch):\n",
      "    print(f'run method start: {__file__}, run({mini_batch})')\n",
      "    resultList = []\n",
      "    in_tensor = g_tf_sess.graph.get_tensor_by_name(\"network/X:0\")\n",
      "    output = g_tf_sess.graph.get_tensor_by_name(\"network/output/MatMul:0\")\n",
      "\n",
      "    for image in mini_batch:\n",
      "        # prepare each image\n",
      "        data = Image.open(image)\n",
      "        np_im = np.array(data).reshape((1, 784))\n",
      "        # perform inference\n",
      "        inference_result = output.eval(feed_dict={in_tensor: np_im}, session=g_tf_sess)\n",
      "        # find best probability, and add to result list\n",
      "        best_result = np.argmax(inference_result)\n",
      "        resultList.append(\"{}: {}\".format(os.path.basename(image), best_result))\n",
      "\n",
      "    return resultList\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scripts_folder = \"Code\"\n",
    "script_file = \"digit_identification.py\"\n",
    "\n",
    "# peek at contents\n",
    "with open(os.path.join(scripts_folder, script_file)) as inference_file:\n",
    "    print(inference_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the batch inference pipeline\n",
    "The data, models, and compute resource are now available. Let's put all these together in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Specify the environment to run the script\n",
    "Specify the conda dependencies for your script. This will allow us to install pip packages as well as configure the inference environment.\n",
    "* Always include **azureml-core** and **azureml-dataset-runtime\\[fuse\\]** in the pip package list to make ParallelRunStep run properly.\n",
    "\n",
    "If you're using custom image (`batch_env.python.user_managed_dependencies = True`), you need to install the package to your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=[\"tensorflow==1.15.2\", \n",
    "                                                          \"pillow\", \n",
    "                                                          \"azureml-core\", \n",
    "                                                          \"azureml-dataset-runtime[fuse]\"])\n",
    "batch_env = Environment(name=\"batch_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the configuration to wrap the inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=scripts_folder,\n",
    "    entry_script=script_file,\n",
    "    mini_batch_size=PipelineParameter(name=\"batch_size_param\", default_value=\"5\"),\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"mnist_outputs.txt\",\n",
    "    environment=batch_env,\n",
    "    compute_target=compute_target,\n",
    "    process_count_per_node=PipelineParameter(name=\"process_count_param\", default_value=2),\n",
    "    node_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline step\n",
    "Create the pipeline step using the script, environment configuration, and parameters. Specify the compute target you already attached to your workspace as the target of execution of the script. We will use ParallelRunStep to create the pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"predict-digits-mnist\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[ input_mnist_ds_consumption ],\n",
    "    output=output_dir,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "At this point you can run the pipeline and examine the output it produced. The Experiment object is used to track the run of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step predict-digits-mnist [5765790c][05c60e21-fea8-42f4-9903-05dd196289fc], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 89e74f80-ade8-4857-8638-ec1813120fbe\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/Lab7-Pipelines/runs/89e74f80-ade8-4857-8638-ec1813120fbe?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/AMLworkshop-rg/workspaces/AMLworkshop\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "experiment = Experiment(ws, 'Lab7-Pipelines')\n",
    "pipeline_run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the run\n",
    "\n",
    "The pipeline run status could be checked in Azure Machine Learning portal (https://ml.azure.com). The link to the pipeline run could be retrieved by inspecting the `pipeline_run` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>Lab7-Pipelines</td><td>89e74f80-ade8-4857-8638-ec1813120fbe</td><td>azureml.PipelineRun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/experiments/Lab7-Pipelines/runs/89e74f80-ade8-4857-8638-ec1813120fbe?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/AMLworkshop-rg/workspaces/AMLworkshop\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: Lab7-Pipelines,\n",
       "Id: 89e74f80-ade8-4857-8638-ec1813120fbe,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will output information of the pipeline run, including the link to the details page of portal.\n",
    "pipeline_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pipeline is Finished at the moment ( 2021-03-22 13:06:50.408284 )\n"
     ]
    }
   ],
   "source": [
    "print(\"The pipeline is\", pipeline_run.get_status(), 'at the moment (', datetime.datetime.now(), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: View detailed logs (streaming) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait the run for completion and show output log to console\n",
    "#pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the prediction results per input image\n",
    "In the digit_identification.py file above you can see that the ResultList with the filename and the prediction result gets returned. These are written to the DataStore specified in the PipelineData object as the output data, which in this case is called *inferences*. This containers the outputs from  all of the worker nodes used in the compute cluster. You can download this data to view the results ... below just filters to the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction has  1000  rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111.png</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114.png</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>102.png</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>103.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>104.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>105.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>106.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Filename  Prediction\n",
       "0  111.png           7\n",
       "1  112.png           3\n",
       "2  113.png           9\n",
       "3  114.png           7\n",
       "4  115.png           4\n",
       "5  102.png           5\n",
       "6  103.png           4\n",
       "7  104.png           9\n",
       "8  105.png           9\n",
       "9  106.png           2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "batch_run = pipeline_run.find_step_run(parallelrun_step.name)[0]\n",
    "batch_output = batch_run.get_output_data(output_dir.name)\n",
    "\n",
    "target_dir = tempfile.mkdtemp()\n",
    "batch_output.download(local_path=target_dir)\n",
    "result_file = os.path.join(target_dir, batch_output.path_on_datastore, parallel_run_config.append_row_file_name)\n",
    "\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"Filename\", \"Prediction\"]\n",
    "print(\"Prediction has \", df.shape[0], \" rows\")\n",
    "df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resubmit a with different dataset\n",
    "Since we made the input a `PipelineParameter`, we can resubmit with a different dataset without having to create an entirely new experiment. We'll use the same datastore but use only a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_on_datastore = mnist_data.path('mnist/0.png')\n",
    "single_image_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted PipelineRun 5353e08a-dcdf-4c02-9483-31c1633df7c2\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/Lab7-Pipelines/runs/5353e08a-dcdf-4c02-9483-31c1633df7c2?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/AMLworkshop-rg/workspaces/AMLworkshop\n"
     ]
    }
   ],
   "source": [
    "pipeline_run_2 = experiment.submit(pipeline, \n",
    "                                   pipeline_parameters={\"mnist_param\": single_image_ds, \n",
    "                                                        \"batch_size_param\": \"1\",\n",
    "                                                        \"process_count_param\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>Lab7-Pipelines</td><td>5353e08a-dcdf-4c02-9483-31c1633df7c2</td><td>azureml.PipelineRun</td><td>Running</td><td><a href=\"https://ml.azure.com/experiments/Lab7-Pipelines/runs/5353e08a-dcdf-4c02-9483-31c1633df7c2?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/AMLworkshop-rg/workspaces/AMLworkshop\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: Lab7-Pipelines,\n",
       "Id: 5353e08a-dcdf-4c02-9483-31c1633df7c2,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: Running)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will output information of the pipeline run, including the link to the details page of portal.\n",
    "pipeline_run_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 5353e08a-dcdf-4c02-9483-31c1633df7c2\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/Lab7-Pipelines/runs/5353e08a-dcdf-4c02-9483-31c1633df7c2?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/AMLworkshop-rg/workspaces/AMLworkshop\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: f9875761-e315-4180-9eac-6bde0eaf0394\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/Lab7-Pipelines/runs/f9875761-e315-4180-9eac-6bde0eaf0394?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/AMLworkshop-rg/workspaces/AMLworkshop\n",
      "StepRun( predict-digits-mnist ) Status: NotStarted\n",
      "StepRun( predict-digits-mnist ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d.txt\n",
      "========================================================================================================================\n",
      "2021-03-22T13:07:17Z Starting output-watcher...\n",
      "2021-03-22T13:07:17Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-03-22T13:07:17Z Executing 'Copy ACR Details file' on 10.0.0.7\n",
      "2021-03-22T13:07:17Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2021-03-22T13:07:17Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "2021-03-22T13:07:18Z Copy ACR Details file succeeded on 10.0.0.7. Output: \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_1f3c26a22a7cca7ea03b17ac7d2bb0a6\n",
      "Digest: sha256:9c1b3720e6155847eb0701e3285ffd89726766aea739759fc97e5d3b59db3313\n",
      "Status: Image is up to date for viennaglobal.azurecr.io/azureml/azureml_1f3c26a22a7cca7ea03b17ac7d2bb0a6:latest\n",
      "viennaglobal.azurecr.io/azureml/azureml_1f3c26a22a7cca7ea03b17ac7d2bb0a6:latest\n",
      "2021-03-22T13:07:19Z Check if container f9875761-e315-4180-9eac-6bde0eaf0394 already exist exited with 0, \n",
      "\n",
      "be3b5894df3396fed7f000fff73a00c8c7fec6a7db5955a44002c980acce994b\n",
      "2021/03/22 13:07:20 Starting App Insight Logger for task:  containerSetup\n",
      "2021/03/22 13:07:20 Version: 3.0.01524.0008 Branch: .SourceBranch Commit: eba29f9\n",
      "2021/03/22 13:07:20 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/03/22 13:07:20 Starting infiniband setup\n",
      "2021/03/22 13:07:20 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/03/22 13:07:20 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/03/22 13:07:20 Starting setupPasswordLessSSH setup\n",
      "2021/03/22 13:07:20 sshd runtime has already been installed in the container\n",
      "2021/03/22 13:07:20 All App Insights Logs was send successfully\n",
      "2021/03/22 13:07:20 App Insight Client has already been closed\n",
      "2021/03/22 13:07:20 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-03-22T13:07:20Z Starting docker container succeeded.\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d.txt\n",
      "===============================================================================================================\n",
      "[2021-03-22T13:07:21.476988] Entering job preparation.\n",
      "[2021-03-22T13:07:22.305506] Starting job preparation.\n",
      "[2021-03-22T13:07:22.305547] Extracting the control code.\n",
      "[2021-03-22T13:07:22.340801] fetching and extracting the control code on master node.\n",
      "[2021-03-22T13:07:22.340858] Starting extract_project.\n",
      "[2021-03-22T13:07:22.340943] Starting to extract zip file.\n",
      "[2021-03-22T13:07:23.041055] Finished extracting zip file.\n",
      "[2021-03-22T13:07:23.178036] Using urllib.request Python 3.0 or later\n",
      "[2021-03-22T13:07:23.178104] Start fetching snapshots.\n",
      "[2021-03-22T13:07:23.178149] Start fetching snapshot.\n",
      "[2021-03-22T13:07:23.178168] Retrieving project from snapshot: 30f660b3-01e6-4099-8aa1-f6f53fae4e31\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 49\n",
      "[2021-03-22T13:07:23.447783] Finished fetching snapshot.\n",
      "[2021-03-22T13:07:23.447829] Start fetching snapshot.\n",
      "[2021-03-22T13:07:23.447857] Retrieving project from snapshot: 076d8c54-083f-4074-ab5a-f7b1e01ee5fd\n",
      "[2021-03-22T13:07:33.153134] Finished fetching snapshot.\n",
      "[2021-03-22T13:07:33.153185] Finished fetching snapshots.\n",
      "[2021-03-22T13:07:33.153194] Finished extract_project.\n",
      "[2021-03-22T13:07:33.164538] Finished fetching and extracting the control code.\n",
      "[2021-03-22T13:07:33.171431] Start run_history_prep.\n",
      "[2021-03-22T13:07:33.440534] Job preparation is complete.\n",
      "[2021-03-22T13:07:33.440923] Entering Data Context Managers in Sidecar\n",
      "[2021-03-22T13:07:33.443226] Running Sidecar prep cmd...\n",
      "[2021-03-22T13:07:33.505261] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/mounts/workspaceblobstore/azureml/f9875761-e315-4180-9eac-6bde0eaf0394\n",
      "[2021-03-22T13:07:33.507020] INFO azureml.sidecar.sidecar: Invoking \"enter_contexts\" task with Context Managers: {\"context_managers\": [\"Dataset:context_managers.Datasets\", \"DataStoreCopy:context_managers.DataStores\"]}\n",
      "Enter __enter__ of DatasetContextManager\n",
      "SDK version: azureml-core==1.22.0 azureml-dataprep==2.10.1. Session id: 367e2375-aa41-4282-ac69-67b57d6ac0d4. Run id: f9875761-e315-4180-9eac-6bde0eaf0394.\n",
      "Processing 'minist_param_config'.\n",
      "Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"('mnist_datastore', 'mnist/0.png')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"1dd36422-4881-4a73-b8e0-00bad2d27187\",\n",
      "    \"name\": null,\n",
      "    \"version\": null,\n",
      "    \"workspace\": \"Workspace.create(name='AMLworkshop', subscription_id='70b8f39e-8863-49f7-b6ba-34a80799550c', resource_group='AMLworkshop-rg')\"\n",
      "  }\n",
      "}\n",
      "Mounting minist_param_config to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/wd/tmpqrnf71ot.\n",
      "Mounted minist_param_config to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/wd/tmpqrnf71ot as single file.\n",
      "Exit __enter__ of DatasetContextManager\n",
      "Set Dataset minist_param_config's target path to /mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/wd/tmpqrnf71ot/0.png\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 1\n",
      "Sidecar adding paths_to_bind: ['/tmp/66a4b715-c681-49fe-9919-b0c815eb1ebc']\n",
      "Acquired lockfile /tmp/f9875761-e315-4180-9eac-6bde0eaf0394-datastore.lock to downloading input data references\n",
      "[2021-03-22T13:07:47.304659] INFO azureml.sidecar.task.enter_contexts: Entered Context Managers\n",
      "[2021-03-22T13:07:48.092355] Ran Sidecar prep cmd.\n",
      "[2021-03-22T13:07:48.092456] Running Context Managers in Sidecar complete.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2021/03/22 13:08:00 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
      "2021/03/22 13:08:00 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "[2021-03-22T13:08:01.546153] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['driver/amlbi_main.py', '--client_sdk_version', '1.24.0', '--scoring_module_name', 'digit_identification.py', '--mini_batch_size', '1', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'mnist_outputs.txt', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/mounts/workspaceblobstore/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/inferences', '--process_count_per_node', '1', '--input_fds_0', 'minist_param_config', '--input_pipeline_param_0', 'DatasetConsumptionConfig:minist_param_config'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 104\n",
      "[2021-03-22T13:08:03.683744] Entering Run History Context Manager.\n",
      "[2021-03-22T13:08:04.910875] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/mounts/workspaceblobstore/azureml/f9875761-e315-4180-9eac-6bde0eaf0394\n",
      "[2021-03-22T13:08:04.911088] Preparing to call script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.24.0', '--scoring_module_name', 'digit_identification.py', '--mini_batch_size', '1', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'mnist_outputs.txt', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/mounts/workspaceblobstore/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/inferences', '--process_count_per_node', '1', '--input_fds_0', 'minist_param_config', '--input_pipeline_param_0', '$minist_param_config']\n",
      "[2021-03-22T13:08:04.911158] After variable expansion, calling script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.24.0', '--scoring_module_name', 'digit_identification.py', '--mini_batch_size', '1', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'mnist_outputs.txt', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/mounts/workspaceblobstore/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/inferences', '--process_count_per_node', '1', '--input_fds_0', 'minist_param_config', '--input_pipeline_param_0', '/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/wd/tmpqrnf71ot/0.png']\n",
      "\n",
      "2021/03/22 13:08:05 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the daemon thread to refresh tokens in background for process with pid = 104\n",
      "\n",
      "\n",
      "[2021-03-22T13:09:01.673706] The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
      "3 items cleaning up...\n",
      "Cleanup took 0.26908278465270996 seconds\n",
      "[2021-03-22T13:09:02.302005] Finished context manager injector.\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d.txt\n",
      "===============================================================================================================\n",
      "[2021-03-22T13:09:04.163567] Entering job release\n",
      "[2021-03-22T13:09:05.768389] Starting job release\n",
      "[2021-03-22T13:09:05.769152] Logging experiment finalizing status in history service.\n",
      "[2021-03-22T13:09:05.769484] job release stage : upload_datastore starting...Starting the daemon thread to refresh tokens in background for process with pid = 270\n",
      "[2021-03-22T13:09:05.770965] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-03-22T13:09:05.773667] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-03-22T13:09:05.773716] job release stage : copy_batchai_cached_logs completed...\n",
      "\n",
      "[2021-03-22T13:09:05.781529] job release stage : execute_job_release starting...\n",
      "[2021-03-22T13:09:05.851445] Entering context manager injector.\n",
      "[2021-03-22T13:09:05.977267] job release stage : send_run_telemetry starting...\n",
      "[2021-03-22T13:09:06.000421] job release stage : upload_datastore completed...\n",
      "[2021-03-22T13:09:06.074510] job release stage : execute_job_release completed...\n",
      "[2021-03-22T13:09:06.176033] get vm size and vm region successfully.\n",
      "[2021-03-22T13:09:06.201035] get compute meta data successfully.\n",
      "[2021-03-22T13:09:06.385994] post artifact meta request successfully.\n",
      "[2021-03-22T13:09:06.409942] upload compute record artifact successfully.\n",
      "[2021-03-22T13:09:06.656902] job release stage : send_run_telemetry completed...\n",
      "[2021-03-22T13:09:06.657422] Running in AzureML-Sidecar, starting to exit user context managers...\n",
      "[2021-03-22T13:09:06.657522] Running Sidecar release cmd...\n",
      "[2021-03-22T13:09:06.672961] INFO azureml.sidecar.sidecar: Received task: exit_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/mounts/workspaceblobstore/azureml/f9875761-e315-4180-9eac-6bde0eaf0394\n",
      "Enter __exit__ of DatasetContextManager\n",
      "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/wd/tmpqrnf71ot.\n",
      "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/amlworkshop/azureml/f9875761-e315-4180-9eac-6bde0eaf0394/wd/tmpqrnf71ot.\n",
      "Exit __exit__ of DatasetContextManager\n",
      "[2021-03-22T13:09:06.901170] Removing absolute paths from host...\n",
      "[2021-03-22T13:09:07.045477] INFO azureml.sidecar.task.exit_contexts: Exited Context Managers\n",
      "[2021-03-22T13:09:08.027840] Ran Sidecar release cmd.\n",
      "[2021-03-22T13:09:08.027990] Job release is complete\n",
      "\n",
      "StepRun(predict-digits-mnist) Execution Summary\n",
      "================================================\n",
      "StepRun( predict-digits-mnist ) Status: Finished\n",
      "{'runId': 'f9875761-e315-4180-9eac-6bde0eaf0394', 'target': 'pipelines76644', 'status': 'Completed', 'startTimeUtc': '2021-03-22T13:07:14.274088Z', 'endTimeUtc': '2021-03-22T13:09:19.646167Z', 'properties': {'ContentSnapshotId': '30f660b3-01e6-4099-8aa1-f6f53fae4e31', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '05c60e21-fea8-42f4-9903-05dd196289fc', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '5765790c', 'azureml.pipelinerunid': '5353e08a-dcdf-4c02-9483-31c1633df7c2', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'azureml.parallelrunstep': 'true'}, 'inputDatasets': [{'dataset': {'id': '1dd36422-4881-4a73-b8e0-00bad2d27187'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'minist_param_config', 'mechanism': 'Mount'}}], 'outputDatasets': [], 'runDefinition': {'script': 'driver/amlbi_main.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--client_sdk_version', '1.24.0', '--scoring_module_name', 'digit_identification.py', '--mini_batch_size', '$AML_PARAMETER_batch_size_param', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'mnist_outputs.txt', '--output', '$AZUREML_DATAREFERENCE_inferences', '--process_count_per_node', '$AML_PARAMETER_process_count_param', '--input_fds_0', 'minist_param_config', '--input_pipeline_param_0', 'DatasetConsumptionConfig:minist_param_config'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'pipelines76644', 'dataReferences': {'inferences': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/f9875761-e315-4180-9eac-6bde0eaf0394/inferences', 'pathOnCompute': None, 'overwrite': False}}, 'data': {'minist_param_config': {'dataLocation': {'dataset': {'id': '1dd36422-4881-4a73-b8e0-00bad2d27187', 'name': None, 'version': None}, 'dataPath': None}, 'mechanism': 'Mount', 'environmentVariableName': 'minist_param_config', 'pathOnCompute': None, 'overwrite': False}}, 'outputData': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 2, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'batch_environment', 'version': 'Autosave_2021-03-09T14:26:28Z_e5fb9939', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['tensorflow==1.15.2', 'pillow', 'azureml-core~=1.24.0', 'azureml-dataset-runtime[fuse]~=1.24.0']}], 'name': 'azureml_d12b2f68dde73178679c9a6bd3d284ca'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AML_PARAMETER_batch_size_param': '1', 'AML_PARAMETER_process_count_param': '1'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210220.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/azureml-logs/55_azureml-execution-tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d.txt?sv=2019-02-02&sr=b&sig=QnVjA2VlfiIpXP5cuV8XA%2BvAAJqwi4Y1u%2FP%2FULdHiyY%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'azureml-logs/55_azureml-execution-tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/azureml-logs/55_azureml-execution-tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d.txt?sv=2019-02-02&sr=b&sig=fP4xrmYSXA2Q5YV%2BcIFmX4OeaSteM0wWgHEH1uXD2dU%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'azureml-logs/65_job_prep-tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/azureml-logs/65_job_prep-tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d.txt?sv=2019-02-02&sr=b&sig=IKqYuA%2B2wQuhX2fw479SE8ufVXxlCiJuHH9dWYlYHD0%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'azureml-logs/65_job_prep-tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/azureml-logs/65_job_prep-tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d.txt?sv=2019-02-02&sr=b&sig=05XLqOIX4043CpzdUnQEVDUIIM9fnAPNovJJz9trofU%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=9y7KRjjPAo%2F%2Fl532OQ8uGohenmpxLtJJrvqFwsIHb6E%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'azureml-logs/75_job_post-tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/azureml-logs/75_job_post-tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d.txt?sv=2019-02-02&sr=b&sig=rHuhVX%2FT2Aq81oTRp1i1vEiznt0IyMJVxNe0IFdheJA%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'azureml-logs/75_job_post-tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/azureml-logs/75_job_post-tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d.txt?sv=2019-02-02&sr=b&sig=jPXMtCu7Bkq4QiYgkBM0m4%2FChXmPt9dZOsi3VxdioKQ%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'azureml-logs/process_info.json': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=cTizBVlXfGfiDSTSic1%2F2dNY%2FVWRMPkn8UkYGl2A57I%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'azureml-logs/process_status.json': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=jipuqo%2BsCdoBWzpeueKhukYOogMLoXMPXweafdrg9Pk%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/104_azureml.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/104_azureml.log?sv=2019-02-02&sr=b&sig=dnLBiV3ZdekbXofWrfcqr2CiCx0EHRkrIWxGIsqR6sI%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/91_azureml.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/91_azureml.log?sv=2019-02-02&sr=b&sig=e9yIiqEe6cP7DpHdtLkZrOJdQ2aZSM3KnAM1owiyrEo%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=sFtAdNi60FuWkWAhEbJ28SbEg75WHxOAWbhWjjpVZ0o%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=XR2tMHri0pj7%2B7cSHe%2FQ86mbHbUHZRAXxVPplqMZVpo%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=t9ogUPSr3WjqLUvzUBRNwI7YSE%2FQof9d8hA7IiigbvY%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=NuWIyuUQiNfu6Ue7Xw%2FAYkTHWwcjRPzBBCeCBHYoNrI%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=BcU9TbacyZAko0zl9G8d1Rp8kqKb1UzVxSQUjHwFHxY%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/sidecar/tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d/all.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/sidecar/tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d/all.log?sv=2019-02-02&sr=b&sig=zD3M%2Ft1bBOdD6vK%2BlJknyAxaOLPu8HYkicGttQdA8vQ%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/sidecar/tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d/task.enter_contexts.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/sidecar/tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d/task.enter_contexts.log?sv=2019-02-02&sr=b&sig=LDHyIFFWGlXUqjdQNEOASazn%2ByMgrNB4%2Fp00xvWfwqU%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/sidecar/tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d/task.exit_contexts.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/sidecar/tvmps_75e067f94c5a2749abec090f80b7c4c42878b1030a1963d115b30244006f6243_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=lMKoUa3oS1PGAwEtROjbtHnIlqMRhd8yiwGammI4L4U%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/sidecar/tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d/all.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/sidecar/tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d/all.log?sv=2019-02-02&sr=b&sig=09eAQ3uCSwYMkL3iJBLaFTVn8UyIBwunZJm6G5CoP38%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/sidecar/tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d/task.enter_contexts.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/sidecar/tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d/task.enter_contexts.log?sv=2019-02-02&sr=b&sig=tn%2F2suEIjSpxU5GNfexgE3II4TP0OaUa8XMWj6drGaM%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/sidecar/tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d/task.exit_contexts.log': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/sidecar/tvmps_e3986445fc76a9fac21720b80dbd69560a45c65a887d03a800d6e351d7cba7a4_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=5eN499hM5NkI68GBgAS2GqAlSakEb8hxcr0onB3DH0s%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=wqybOQrMjvYoN7Kdjk%2BamG0axHHOSRlDI980azWBWQU%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.f9875761-e315-4180-9eac-6bde0eaf0394/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=nqvZvW7lsQJ7PSKZKVjGkkcuNykH5G7OcFjRO2SLm5A%3D&st=2021-03-22T12%3A59%3A11Z&se=2021-03-22T21%3A09%3A11Z&sp=r'}, 'submittedBy': 'Serge Retkowsky'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '5353e08a-dcdf-4c02-9483-31c1633df7c2', 'status': 'Completed', 'startTimeUtc': '2021-03-22T13:06:56.78615Z', 'endTimeUtc': '2021-03-22T13:09:22.335293Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{\"batch_size_param\":\"1\",\"process_count_param\":\"1\"}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.5353e08a-dcdf-4c02-9483-31c1633df7c2/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=D7gSp%2B3p46YJUIZ4sQt9IvHICaJLev9xcg3c6sYd7SA%3D&st=2021-03-22T12%3A59%3A24Z&se=2021-03-22T21%3A09%3A24Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.5353e08a-dcdf-4c02-9483-31c1633df7c2/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=9dk5HBUgjqrcINQFGU8hRAxOXpJoLMoYa0jbrBNupgI%3D&st=2021-03-22T12%3A59%3A24Z&se=2021-03-22T21%3A09%3A24Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlworkshop1458610383.blob.core.windows.net/azureml/ExperimentRun/dcid.5353e08a-dcdf-4c02-9483-31c1633df7c2/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=vb1XkiHMkCSWbRenoEKaIPj6Ho1vI7J1a2tgSa3MgGU%3D&st=2021-03-22T12%3A59%3A24Z&se=2021-03-22T21%3A09%3A24Z&sp=r'}, 'submittedBy': 'Serge Retkowsky'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait the run for completion and show output log to console\n",
    "pipeline_run_2.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pipeline is Finished at the moment ( 2021-03-22 13:09:24.441993 )\n"
     ]
    }
   ],
   "source": [
    "print(\"The pipeline is\", pipeline_run_2.get_status(), 'at the moment (', datetime.datetime.now(), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Compute resources\n",
    "\n",
    "For re-occurring jobs, it may be wise to keep compute the compute resources and allow compute nodes to scale down to 0. However, since this is just a single-run job, we are free to release the allocated compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of labs"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "joringer"
   },
   {
    "name": "asraniwa"
   },
   {
    "name": "pansav"
   },
   {
    "name": "tracych"
   }
  ],
  "category": "Other notebooks",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "None"
  ],
  "friendly_name": "MNIST data inferencing using ParallelRunStep",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "tags": [
   "Batch Inferencing",
   "Pipeline"
  ],
  "task": "Digit identification"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
